{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "\n",
    "from utils import plot_pca\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>ACCOAC</th>\n",
       "      <th>MDH</th>\n",
       "      <th>PTAr</th>\n",
       "      <th>CS</th>\n",
       "      <th>ACACT1r</th>\n",
       "      <th>PPC</th>\n",
       "      <th>PPCK</th>\n",
       "      <th>PFL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Strain 1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strain 2</th>\n",
       "      <td>0.552101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strain 3</th>\n",
       "      <td>0.349196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value  ACCOAC  MDH  PTAr   CS  ACACT1r  PPC  PPCK  PFL\n",
       "Line Name                                                           \n",
       "Strain 1   0.000000     1.0  1.0   2.0  0.0      2.0  0.0   0.0  0.0\n",
       "Strain 2   0.552101     1.0  2.0   2.0  2.0      2.0  1.0   1.0  0.0\n",
       "Strain 3   0.349196     1.0  0.0   0.0  2.0      1.0  1.0   2.0  0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../data/EDD_isoprenol_production.csv', index_col=0)\n",
    "df.drop('Measurement Type', axis=1, inplace=True)\n",
    "\n",
    "# Split the data into X and y\n",
    "X = df.drop('Value', axis=1).copy()\n",
    "X = X.astype('int64')\n",
    "y = df['Value'].copy()\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom PCA on the data\n",
    "pca = PCA(n_components=2)\n",
    "pca_df = pd.DataFrame(pca.fit_transform(df.drop('Value', axis=1)), index=df.index, columns=['PC1', 'PC2'])\n",
    "pca_df.index = df.index\n",
    "pca_df['Value'] = df['Value']\n",
    "\n",
    "# Plot the PCA\n",
    "plot_pca(pca_df, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and tune an XGBoost model using optuna\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 5, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 25),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 10, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda',1e-4, 1.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor(**params)\n",
    "    scores = cross_val_score(xgb, X, y, scoring='neg_root_mean_squared_error', cv=3, n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Train the model on the entire dataset\n",
    "params = study.best_trial.params\n",
    "xgb_model = XGBRegressor(**params)\n",
    "xgb_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate a Linear Regression model on the data\n",
    "def train(model, X, y):\n",
    "    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)\n",
    "    print(f'RMSE = {np.round(np.sqrt(np.abs(scores.mean())),4)}')\n",
    "    print(f'STD = {np.round(scores.std(),4)}')\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def plot_R2(model, X, y):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.scatter(y, model.predict(X), color='blue', alpha=0.5)\n",
    "    ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title('Cross Validation Predictions')\n",
    "    plt.show()\n",
    "\n",
    "xgb = train(xgb_model, X, y)\n",
    "plot_R2(xgb, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_plots(model, X):\n",
    "    # Calculate SHAP values and plot\n",
    "    explainer = shap.TreeExplainer(model, X)\n",
    "    shap_values = explainer(X)\n",
    "    shap.summary_plot(shap_values, X, plot_type='bar')\n",
    "    shap.summary_plot(shap_values, X, plot_type='dot')\n",
    "    order = np.argsort(model.predict(X))\n",
    "    shap.plots.heatmap(shap_values, instance_order=order)\n",
    "    return explainer, shap_values\n",
    "\n",
    "explainer, shap_values = shap_plots(xgb, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap_df = pd.DataFrame(shap_values.values, columns=X.columns)\n",
    "shap_df.index = df.index\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "model = KMeans(n_init='auto')\n",
    "visualizer = KElbowVisualizer(model, k=(2,10))\n",
    "visualizer.fit(shap_df)\n",
    "visualizer.show()\n",
    "\n",
    "# Get optimal number of clusters\n",
    "n_clusters = visualizer.elbow_value_\n",
    "print(f'Optimal number of clusters = {n_clusters}')\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42).fit(shap_df)\n",
    "shap_df['Cluster'] = kmeans.labels_\n",
    "shap_df['Value'] = df['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = shap_df['Cluster']\n",
    "# groupy by cluster and calculate most common values\n",
    "display(df.groupby('Cluster').agg(lambda x:x.value_counts().index[0]))\n",
    "# groupy by cluster and calculate mean values\n",
    "display(df.groupby('Cluster').mean())\n",
    "# groupy by cluster and calculate median values\n",
    "display(df.groupby('Cluster').median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Cluster'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDP & ICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping for the df columns\n",
    "mapping = pd.DataFrame(columns=['Feature', 'index'])\n",
    "mapping['Feature'] = X.columns\n",
    "mapping['index'] = [i for i in range(len(X.columns))]\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "col = 'CS'\n",
    "idx = mapping[mapping['Feature'] == col]['index'].values[0]\n",
    "PartialDependenceDisplay.from_estimator(xgb, X, [idx], kind='both', centered=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from interpret import set_visualize_provider, show\n",
    "# from interpret.blackbox import PartialDependence\n",
    "# from interpret.provider import InlineProvider\n",
    "# set_visualize_provider(InlineProvider())\n",
    "\n",
    "# pdp = PartialDependence(rf, X)\n",
    "# show(pdp.explain_global(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index'] = [i for i in range(len(df))]\n",
    "df.sort_values(by='Value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [i for i in range(len(X.columns))]\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X.values, feature_names=X.columns.values.tolist(),\n",
    "                                                  verbose=True, mode='regression',  \n",
    "                                                   categorical_features=categorical_features)\n",
    "j = 6\n",
    "exp = explainer.explain_instance(X.values[j], xgb.predict, num_features=8)\n",
    "exp.show_in_notebook(show_table=True, show_all=True)\n",
    "pd.DataFrame(exp.as_list(), columns=['Feature', 'Contribution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OMLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # suppress CUDA warnings from tensorflow\n",
    "\n",
    "# import the necessary packages\n",
    "from omlt import OmltBlock, OffsetScaling\n",
    "from omlt.io.keras import load_keras_sequential\n",
    "from omlt.neuralnet import ReluBigMFormulation\n",
    "import pyomo.environ as pyo\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/EDD_isoprenol_production.csv', index_col=0)\n",
    "df.drop('Measurement Type', axis=1, inplace=True)\n",
    "X = df.drop('Value', axis=1).copy()\n",
    "y = df['Value'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X.columns.values.tolist()\n",
    "outputs = ['Value']\n",
    "\n",
    "dfin = df[inputs]\n",
    "dfout = df[outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We scale the data for improved training, however, we want to formulate\n",
    "# our optimizaton problem on the original variables. Therefore, we keep\n",
    "# the scaling parameters to use later in our optimization formulation\n",
    "\n",
    "x_offset, x_factor = dfin.mean().to_dict(), dfin.std().to_dict()\n",
    "y_offset, y_factor = dfout.mean().to_dict(), dfout.std().to_dict()\n",
    "\n",
    "dfin = (dfin - dfin.mean()).divide(dfin.std())\n",
    "dfout = (dfout - dfout.mean()).divide(dfout.std())\n",
    "\n",
    "# capture the minimum and maximum values of the scaled inputs\n",
    "# so we don't use the model outside the valid range\n",
    "scaled_lb = dfin.min()[inputs].values\n",
    "scaled_ub = dfin.max()[inputs].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our Keras Sequential model\n",
    "nn = Sequential(name='reformer_relu_4_20')\n",
    "nn.add(Dense(units=50, input_dim=len(inputs), activation='relu'))\n",
    "# nn.add(Dense(units=248, activation='relu'))\n",
    "# nn.add(Dense(units=10, activation='relu'))\n",
    "# nn.add(Dense(units=10, activation='relu'))\n",
    "nn.add(Dense(units=len(outputs)))\n",
    "nn.compile(optimizer=Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfin.values\n",
    "y = dfout.values\n",
    "\n",
    "history = nn.fit(X, y, epochs=100, validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function over time\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create the Pyomo model\n",
    "m = pyo.ConcreteModel()\n",
    "# create the OmltBlock to hold the neural network model\n",
    "m.reformer = OmltBlock()\n",
    "\n",
    "# load the Keras model\n",
    "# nn_reformer = keras.models.load_model('reformer_nn_relu', compile=False)\n",
    "nn_reformer = nn\n",
    "\n",
    "# Note: The neural network is in the scaled space. We want access to the\n",
    "# variables in the unscaled space. Therefore, we need to tell OMLT about the\n",
    "# scaling factors\n",
    "scaler = OffsetScaling(\n",
    "        offset_inputs={i: x_offset[inputs[i]] for i in range(len(inputs))},\n",
    "        factor_inputs={i: x_factor[inputs[i]] for i in range(len(inputs))},\n",
    "        offset_outputs={i: y_offset[outputs[i]] for i in range(len(outputs))},\n",
    "        factor_outputs={i: y_factor[outputs[i]] for i in range(len(outputs))}\n",
    "    )\n",
    "\n",
    "scaled_input_bounds = {i: (scaled_lb[i], scaled_ub[i]) for i in range(len(inputs))}\n",
    "\n",
    "# create a network definition from the Keras model\n",
    "net = load_keras_sequential(nn_reformer, scaling_object=scaler, scaled_input_bounds=scaled_input_bounds)\n",
    "\n",
    "# create the variables and constraints for the neural network in Pyomo\n",
    "m.reformer.build_formulation(ReluBigMFormulation(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the objective and constraints\n",
    "iso_idx = outputs.index('Value')\n",
    "m.obj = pyo.Objective(expr=m.reformer.outputs[iso_idx], sense=pyo.maximize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now solve the optimization problem (this may take some time)\n",
    "solver = pyo.SolverFactory('gurobi')\n",
    "status = solver.solve(m, tee=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(inputs)):\n",
    "    print(inputs[i], np.round(pyo.value(m.reformer.inputs[i])))\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    print('***')\n",
    "    print(outputs[i], pyo.value(m.reformer.outputs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
